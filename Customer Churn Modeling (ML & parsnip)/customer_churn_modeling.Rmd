---
title: "Customer Churn Modeling using ML with parsnip"
author: "Albert Campillo"
date: "5/21/2020"
output: 
    html_document: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE)
```


# CHURN MODELING USING MACHINE LEARNING

Considerations:
- The case study attempts to tackle a __classification__ problem:
- It uses `tidymodels` (and `parsnip` in particular) to create & execute a tidy modelling workflow.
- Goal: demonstrate the easiness to fit a simple logistic regression in R's `glm` and quickly switch to a __cross-validated random forest__ using the `ranger` engine by changing a few lines of code.
- Data exploration & wrangling are kept to a minimum (tough they are paramount steps, in this case study, the modeling workflow is the main focus).

# WORKFLOW

The main steps in my workflow:
1. Start with raw data in CSV format
2. Use `skimr` to quickly understand the features
3. Use `rsample` to split into training/ test sets
4. Use `recipes` to create data preprocessing pipelines
5. Use `parsnip`, `rsample` and `yardstick` to build models & assess ML performance


# 1.0. Setup libraries & load dataset

## Packages

Mainly using several libraries within `tidymodels` package: 
- `parsnip`: ML API in R, similar to scikit learn in Python,
- `rsample`: 10-fold cross-validation,
- `recipes`: data preprocessing,
- `yardstick`: model scoring & metrics

```{r}
library(tidyverse)  # loads `dplyr`, `ggplot2`, `purrr`, ...
library(tidymodels) # loads `parsnip`, `rsample`, `recipes`, `yardstick`
library(skimr)      # quickly get a sense of the data
library(knitr)      # pretty html tables
library(ranger)     # random forest library (used for churn modeling)
```

## Dataset
Source: Telco Customer Churn from IBM Watson Analytics
Data structure: 7,043 rows (each representing a customer), 21 columns, prospective predictions to forecaset customer behaviour and help developed focused customer retention programmes.

__Churn__ is the __dependent variable__; it shows customers who left within the last month.
The dataset comprises also details on the __services__ each customer signed up for, along with __account__ and __demographics__ information


```{r}
path <- "data/WA_Fn-UseC_-Telco-Customer-Churn.csv"
telco <- read_csv(path)

telco %>% head()
```

# 2.0. Skim the data

Get a sense of the data using `skim()` from `skimr` package.

Data considerations:
- __customerID__ is a unique identifier and has no descriptive or predictive power. Actions: remove from list of predictors
- Very small number of missing value in __TotalCharges__ variable (11 obs). Action: remove these observations

```{r}
telco %>% skim()
```

```{r}
telco <- telco %>%
    select(-customerID) %>%
    drop_na()
```


# 3.0. Tidymodels Workflow - Generalized Linear Model (Baseline)

I start by fitting & evaluating a simple regression model in order to illustrate the basic steps in the `tidymodels` framework.

## 3.1. Train/ test split
`rsample` provides a streamlined way to create a randomised training & test split of the original data
I split a train set & test set with a proportion 80%-20% of the original data, and assign each as `train_tbl` and `test_tbl` respectively. 

```{r}
set.seed(seed = 1972)

# split original data into train set (80%) and test set (20%)
train_test_split <-
    rsample::initial_split(
        data = telco,
        prop = 0.80
    )

train_test_split

# save each split into train and set tibbles
train_tbl <- train_test_split %>% training()
test_tbl  <- train_test_split %>% testing()
```

## 3.2. Data preparation with recipes

`recipes` uses a __cooking metaphor__ to handle all the data preprocessing (i.e. missing imputation, removing predictors, centric, scaling, one-hot-encoding,...)

Steps:
- 1) First, create the recipe where I define the transformations to apply to the data. My main transformation is switching character variables into factors
- Then, I prepare the recipe by mixing the ingredients with the `prep()` function.

```{r}
reciple_simple <- function(dataset) {
    recipe(Churn ~ ., data = dataset) %>%
        step_string2factor(all_nominal(), -all_outcomes()) %>%
        prep(data = dataset)
}
```

- 3) In order to avoid __Data Leakage__ (transferring information from the train set to the test set), data should be prepped using train_tbl only

```{r}
recipe_prepped <-  reciple_simple(dataset = train_tbl)
```

- 4) Finally, continuing with the cooking metaphor, I bake the recipe to apply all preprocessing to the data sets
```{r}
train_baked <- bake(recipe_prepped, new_data = train_tbl)
test_baked  <- bake(recipe_prepped, new_data = test_tbl)
```


## 3.3. Machine Learning & Performance

__Fit the Model__
`parsnip` is part of `tidymodels` suite. It offers a unified API that allows access to several ML packages without the need to learn the syntax of each individual one.

In 3 steps you can:
1) Set the type of model to fit (i.e. a logistic regression) and its mode (i.e. classification)
2) Choose the computation engine to use (i.e. `glm`). (For logistic regression, `glm`, `glmnet`, `stan`, `spark`, `keras`)
3) spell out the exact model specification to fit (i.e. use all variables) and what data to use (i.e. the baked train set)

```{r}
logistic_glm <- logistic_reg(mode = "classification") %>%
    set_engine("glm") %>%
    fit(Churn ~ ., data = train_baked)
```


*Assess Performance*

```{r}
predictions_glm <- logistic_glm %>%
    predict(new_data = test_baked) %>%
    bind_cols(test_baked %>% select(Churn))

predictions_glm %>% head()
```







